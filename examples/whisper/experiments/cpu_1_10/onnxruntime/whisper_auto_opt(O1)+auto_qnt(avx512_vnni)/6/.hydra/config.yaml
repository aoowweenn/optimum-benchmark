backend:
  name: onnxruntime
  version: 1.15.0
  inter_op_num_threads: null
  intra_op_num_threads: null
  _target_: src.backend.onnxruntime.ORTBackend
  provider: ${infer_provider:${device}}
  use_io_binding: ${is_gpu:${device}}
  enable_profiling: ${benchmark.profile}
  optimization: false
  optimization_config:
    optimization_level: 1
    optimize_for_gpu: ${is_gpu:${device}}
    fp16: false
    enable_transformers_specific_optimizations: true
    enable_gelu_approximation: false
    disable_gelu_fusion: false
    disable_layer_norm_fusion: false
    disable_attention_fusion: false
    disable_skip_layer_norm_fusion: true
    disable_bias_skip_layer_norm_fusion: false
    disable_bias_gelu_fusion: false
    use_mask_index: false
    no_attention_mask: false
    disable_embed_layer_norm_fusion: true
    disable_shape_inference: false
    use_multi_head_attention: false
    enable_gemm_fast_gelu_fusion: false
    use_raw_attention_mask: false
    disable_group_norm_fusion: true
    disable_packed_kv: true
  auto_optimization: O1
  auto_optimization_config:
    for_gpu: ${is_gpu:${device}}
  quantization: false
  quantization_config:
    is_static: false
    format: QOperator
    mode: IntegerOps
    activations_dtype: QUInt8
    activations_symmetric: false
    weights_dtype: QInt8
    weights_symmetric: true
    per_channel: false
    reduce_range: false
    operators_to_quantize:
    - MatMul
    - Add
  auto_quantization: avx512_vnni
  auto_quantization_config:
    is_static: false
    per_channel: false
    operators_to_quantize:
    - Gather
    - Transpose
    - EmbedLayerNormalization
    - Attention
    - LSTM
    - ArgMax
    - Gemm
    - MatMul
    - Add
    - Mul
    - Relu
    - Clip
    - LeakyRelu
    - Sigmoid
    - MaxPool
    - GlobalAveragePool
    - Split
    - Pad
    - Reshape
    - Squeeze
    - Unsqueeze
    - Resize
    - AveragePool
    - Concat
    - Softmax
    - Where
    - ConvTranspose
    - InstanceNormalization
benchmark:
  name: inference
  _target_: src.benchmark.inference.InferenceBenchmark
  seed: 42
  memory: false
  profile: false
  warmup_runs: 10
  benchmark_duration: 10
  batch_size: 1
  new_tokens: 10
experiment_name: whisper_auto_opt(${backend.auto_optimization})+auto_qnt(${backend.auto_quantization})
model: openai/whisper-base
device: cpu
task: ${infer_task:${model}}
environment:
  optimum_version: 1.8.8.dev0
  transformers_version: 4.29.2
  python_version: 3.10.12
  system: Linux
  cpu: ' Intel(R) Xeon(R) CPU @ 2.20GHz'
  cpu_count: 2
  cpu_ram_mb: 12982
  gpu: CUDA not available
  gpu_vram_mb: CUDA not available

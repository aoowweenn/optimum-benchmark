experiment_name,backend._target_,backend.name,backend.version,backend.provider,backend.use_io_binding,backend.enable_profiling,backend.inter_op_num_threads,backend.intra_op_num_threads,backend.optimization,backend.optimization_config.optimization_level,backend.optimization_config.optimize_for_gpu,backend.optimization_config.fp16,backend.optimization_config.enable_transformers_specific_optimizations,backend.optimization_config.enable_gelu_approximation,backend.optimization_config.disable_gelu_fusion,backend.optimization_config.disable_layer_norm_fusion,backend.optimization_config.disable_attention_fusion,backend.optimization_config.disable_skip_layer_norm_fusion,backend.optimization_config.disable_bias_skip_layer_norm_fusion,backend.optimization_config.disable_bias_gelu_fusion,backend.optimization_config.use_mask_index,backend.optimization_config.no_attention_mask,backend.optimization_config.disable_embed_layer_norm_fusion,backend.optimization_config.disable_shape_inference,backend.optimization_config.use_multi_head_attention,backend.optimization_config.enable_gemm_fast_gelu_fusion,backend.optimization_config.use_raw_attention_mask,backend.optimization_config.disable_group_norm_fusion,backend.optimization_config.disable_packed_kv,backend.auto_optimization,backend.auto_optimization_config.for_gpu,backend.quantization,backend.quantization_config.is_static,backend.quantization_config.format,backend.quantization_config.mode,backend.quantization_config.activations_dtype,backend.quantization_config.activations_symmetric,backend.quantization_config.weights_dtype,backend.quantization_config.weights_symmetric,backend.quantization_config.per_channel,backend.quantization_config.reduce_range,backend.auto_quantization,backend.auto_quantization_config.is_static,benchmark._target_,benchmark.name,benchmark.profile,benchmark.memory,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.batch_size,benchmark.new_tokens,model,task,device,experiment_datetime,environment.optimum_version,environment.transformers_version,environment.python_version,environment.system,environment.architecture,environment.machine,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,environment.gpu_ram_mb,forward.latency(s),forward.throughput(iter/s),generate.throughput(tok/s),backend.disable_grad,backend.eval_mode,backend.bettertransformer,backend.torch_compile,backend.fp16,Model.Speedup(%),Generation.Speedup(%),experiment_name
whisper_auto_opt(O4),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1.0,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O4,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.058,17.2,1130.0,,,,,,460.26058631921825,79.93630573248407,whisper_auto_opt(O4)
whisper_auto_opt(O3),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1.0,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O3,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.248,4.04,998.0,,,,,,31.596091205211742,58.917197452229296,whisper_auto_opt(O3)
whisper_auto_opt(O2),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1.0,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O2,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.249,4.02,1010.0,,,,,,30.944625407166114,60.828025477707,whisper_auto_opt(O2)
whisper_auto_opt(O1),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1.0,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O1,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.329,3.04,876.0,,,,,,-0.9771986970683932,39.4904458598726,whisper_auto_opt(O1)
whisper_baseline_with_fp16,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:47:49,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.164,6.09,718.0,True,True,False,False,True,98.37133550488599,14.33121019108281,whisper_baseline_with_fp16
whisper_baseline,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:45:32,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0.326,3.07,628.0,True,True,False,False,False,0.0,0.0,whisper_baseline

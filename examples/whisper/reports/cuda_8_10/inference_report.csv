experiment_name,backend._target_,backend.name,backend.version,backend.provider,backend.use_io_binding,backend.enable_profiling,backend.inter_op_num_threads,backend.intra_op_num_threads,backend.optimization,backend.optimization_config.optimization_level,backend.optimization_config.optimize_for_gpu,backend.optimization_config.fp16,backend.optimization_config.enable_transformers_specific_optimizations,backend.optimization_config.enable_gelu_approximation,backend.optimization_config.disable_gelu_fusion,backend.optimization_config.disable_layer_norm_fusion,backend.optimization_config.disable_attention_fusion,backend.optimization_config.disable_skip_layer_norm_fusion,backend.optimization_config.disable_bias_skip_layer_norm_fusion,backend.optimization_config.disable_bias_gelu_fusion,backend.optimization_config.use_mask_index,backend.optimization_config.no_attention_mask,backend.optimization_config.disable_embed_layer_norm_fusion,backend.optimization_config.disable_shape_inference,backend.optimization_config.use_multi_head_attention,backend.optimization_config.enable_gemm_fast_gelu_fusion,backend.optimization_config.use_raw_attention_mask,backend.optimization_config.disable_group_norm_fusion,backend.optimization_config.disable_packed_kv,backend.auto_optimization,backend.auto_optimization_config.for_gpu,backend.quantization,backend.quantization_config.is_static,backend.quantization_config.format,backend.quantization_config.mode,backend.quantization_config.activations_dtype,backend.quantization_config.activations_symmetric,backend.quantization_config.weights_dtype,backend.quantization_config.weights_symmetric,backend.quantization_config.per_channel,backend.quantization_config.reduce_range,backend.quantization_config.operators_to_quantize,backend.auto_quantization,backend.auto_quantization_config.is_static,benchmark._target_,benchmark.name,benchmark.profile,benchmark.memory,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.batch_size,benchmark.new_tokens,model,task,device,experiment_datetime,environment.optimum_version,environment.transformers_version,environment.python_version,environment.system,environment.architecture,environment.machine,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,environment.gpu_ram_mb,Unnamed: 0,forward.latency(s),forward.throughput(iter/s),generate.throughput(tok/s),backend.disable_grad,backend.eval_mode,backend.bettertransformer,backend.torch_compile,backend.fp16,forward.speedup(%),generate.speedup(%)
whisper_auto_opt(O4),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O4,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.0585,17.1,706.0,,,,,,419.7568389057751,237.79904306220095
whisper_baseline_with_fp16,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:47:49,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.167,6.0,311.0,True,True,False,False,True,82.37082066869301,48.80382775119618
whisper_auto_opt(O3),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O3,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.251,3.99,270.0,,,,,,21.27659574468086,29.186602870813406
whisper_auto_opt(O2),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O2,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.253,3.96,270.0,,,,,,20.364741641337393,29.186602870813406
whisper_auto_opt(O1),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O1,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.33,3.03,205.0,,,,,,-7.902735562310037,-1.9138755980861233
whisper_baseline,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,8,10,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:45:32,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.304,3.29,209.0,True,True,False,False,False,0.0,0.0

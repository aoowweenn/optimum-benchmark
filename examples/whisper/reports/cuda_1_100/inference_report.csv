experiment_name,backend.name,backend.version,backend.inter_op_num_threads,backend.intra_op_num_threads,backend._target_,backend.provider,backend.use_io_binding,backend.enable_profiling,backend.optimization,backend.optimization_config.optimization_level,backend.optimization_config.optimize_for_gpu,backend.optimization_config.fp16,backend.optimization_config.enable_transformers_specific_optimizations,backend.optimization_config.enable_gelu_approximation,backend.optimization_config.disable_gelu_fusion,backend.optimization_config.disable_layer_norm_fusion,backend.optimization_config.disable_attention_fusion,backend.optimization_config.disable_skip_layer_norm_fusion,backend.optimization_config.disable_bias_skip_layer_norm_fusion,backend.optimization_config.disable_bias_gelu_fusion,backend.optimization_config.use_mask_index,backend.optimization_config.no_attention_mask,backend.optimization_config.disable_embed_layer_norm_fusion,backend.optimization_config.disable_shape_inference,backend.optimization_config.use_multi_head_attention,backend.optimization_config.enable_gemm_fast_gelu_fusion,backend.optimization_config.use_raw_attention_mask,backend.optimization_config.disable_group_norm_fusion,backend.optimization_config.disable_packed_kv,backend.auto_optimization,backend.auto_optimization_config.for_gpu,backend.quantization,backend.quantization_config.is_static,backend.quantization_config.format,backend.quantization_config.mode,backend.quantization_config.activations_dtype,backend.quantization_config.activations_symmetric,backend.quantization_config.weights_dtype,backend.quantization_config.weights_symmetric,backend.quantization_config.per_channel,backend.quantization_config.reduce_range,backend.quantization_config.operators_to_quantize,backend.auto_quantization,backend.auto_quantization_config.is_static,benchmark.name,benchmark._target_,benchmark.seed,benchmark.memory,benchmark.profile,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.batch_size,benchmark.new_tokens,model,device,task,environment.optimum_version,environment.transformers_version,environment.python_version,environment.system,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,environment.gpu_vram_mb,Unnamed: 0,forward.latency(s),forward.throughput(samples/s),generate.latency(s),generate.throughput(tokens/s),backend.disable_grad,backend.eval_mode,backend.fp16,backend.bettertransformer,backend.torch_compile,forward.speedup(%),generate.speedup(%)
whisper_auto_opt(O4),onnxruntime,1.15.0,,,src.backend.onnxruntime.ORTBackend,CUDAExecutionProvider,True,False,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O4,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.00985,102.0,0.403,248.0,,,,,,314.6341463414634,78.41726618705036
whisper_baseline_with_fp16,pytorch,2.0.1+cu118,,,src.backend.pytorch.PyTorchBackend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.0294,34.0,0.771,130.0,False,False,True,False,False,38.21138211382114,-6.474820143884896
whisper_auto_opt(O2),onnxruntime,1.15.0,,,src.backend.onnxruntime.ORTBackend,CUDAExecutionProvider,True,False,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O2,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.0345,29.0,0.405,247.0,,,,,,17.886178861788604,77.6978417266187
whisper_auto_opt(O3),onnxruntime,1.15.0,,,src.backend.onnxruntime.ORTBackend,CUDAExecutionProvider,True,False,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O3,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.0346,28.9,0.425,236.0,,,,,,17.479674796747947,69.78417266187051
whisper_auto_opt(O1),onnxruntime,1.15.0,,,src.backend.onnxruntime.ORTBackend,CUDAExecutionProvider,True,False,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O1,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.0425,23.5,0.491,204.0,,,,,,-4.471544715447163,46.76258992805755
whisper_baseline,pytorch,2.0.1+cu118,,,src.backend.pytorch.PyTorchBackend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,inference,src.benchmark.inference.InferenceBenchmark,42,False,False,10,10,1,100,openai/whisper-base,cuda,automatic-speech-recognition,1.8.8.dev0,4.29.2,3.10.12,Linux, Intel(R) Xeon(R) CPU @ 2.30GHz,2,12982,Tesla T4,15360,0,0.0406,24.6,0.718,139.0,False,False,False,False,False,0.0,0.0

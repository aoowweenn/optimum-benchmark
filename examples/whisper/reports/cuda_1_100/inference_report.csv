experiment_name,backend._target_,backend.name,backend.version,backend.provider,backend.use_io_binding,backend.enable_profiling,backend.inter_op_num_threads,backend.intra_op_num_threads,backend.optimization,backend.optimization_config.optimization_level,backend.optimization_config.optimize_for_gpu,backend.optimization_config.fp16,backend.optimization_config.enable_transformers_specific_optimizations,backend.optimization_config.enable_gelu_approximation,backend.optimization_config.disable_gelu_fusion,backend.optimization_config.disable_layer_norm_fusion,backend.optimization_config.disable_attention_fusion,backend.optimization_config.disable_skip_layer_norm_fusion,backend.optimization_config.disable_bias_skip_layer_norm_fusion,backend.optimization_config.disable_bias_gelu_fusion,backend.optimization_config.use_mask_index,backend.optimization_config.no_attention_mask,backend.optimization_config.disable_embed_layer_norm_fusion,backend.optimization_config.disable_shape_inference,backend.optimization_config.use_multi_head_attention,backend.optimization_config.enable_gemm_fast_gelu_fusion,backend.optimization_config.use_raw_attention_mask,backend.optimization_config.disable_group_norm_fusion,backend.optimization_config.disable_packed_kv,backend.auto_optimization,backend.auto_optimization_config.for_gpu,backend.quantization,backend.quantization_config.is_static,backend.quantization_config.format,backend.quantization_config.mode,backend.quantization_config.activations_dtype,backend.quantization_config.activations_symmetric,backend.quantization_config.weights_dtype,backend.quantization_config.weights_symmetric,backend.quantization_config.per_channel,backend.quantization_config.reduce_range,backend.quantization_config.operators_to_quantize,backend.auto_quantization,backend.auto_quantization_config.is_static,benchmark._target_,benchmark.name,benchmark.profile,benchmark.memory,benchmark.warmup_runs,benchmark.benchmark_duration,benchmark.batch_size,benchmark.new_tokens,model,task,device,experiment_datetime,environment.optimum_version,environment.transformers_version,environment.python_version,environment.system,environment.architecture,environment.machine,environment.cpu,environment.cpu_count,environment.cpu_ram_mb,environment.gpu,environment.gpu_ram_mb,Unnamed: 0,forward.latency(s),forward.throughput(iter/s),generate.throughput(tok/s),backend.disable_grad,backend.eval_mode,backend.bettertransformer,backend.torch_compile,backend.fp16,forward.speedup(%),generate.speedup(%)
whisper_auto_opt(O4),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O4,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.00934,107.0,255.0,,,,,,338.5245901639345,65.5844155844156
whisper_baseline_with_fp16,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:47:49,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.0304,32.9,113.0,True,True,False,False,True,34.836065573770504,-26.623376623376625
whisper_auto_opt(O3),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O3,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.0349,28.7,228.0,,,,,,17.622950819672134,48.05194805194806
whisper_auto_opt(O2),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O2,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.035,28.5,234.0,,,,,,16.803278688524603,51.94805194805194
whisper_auto_opt(O1),src.backend.onnxruntime.ORTBackend,onnxruntime,1.15.0,CUDAExecutionProvider,True,False,,,False,1,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False,False,True,True,O1,True,False,False,QOperator,IntegerOps,QUInt8,False,QInt8,True,False,False,"['MatMul', 'Add']",,False,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:49:56,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.0412,24.3,225.0,,,,,,-0.4098360655737654,46.10389610389611
whisper_baseline,src.backend.pytorch.PyTorchBackend,pytorch,2.0.1+cu118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,src.benchmark.inference.InferenceBenchmark,inference,False,False,10,10,1,100,openai/whisper-base,automatic-speech-recognition,cuda,2023-06-07_13:45:32,1.8.7.dev0,4.29.2,3.10.11,Linux,64bit,x86_64,x86_64,2,12982,Tesla T4,15360,0,0.0409,24.4,154.0,True,True,False,False,False,0.0,0.0
